{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SummCNNDM2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNqcO8lhXi7PBfqAHy53KOG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"eCTzOvKx6CMC","colab_type":"code","colab":{}},"source":["function ConnectButton(){\n","    console.log(\"Connect pushed\"); \n","    document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click() \n","}\n","setInterval(ConnectButton,60000);"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3X8pnBQK71dg","colab_type":"code","outputId":"bf6518f8-2864-4fba-9fe2-0ddcecdff7d6","executionInfo":{"status":"ok","timestamp":1588261729037,"user_tz":-330,"elapsed":3403,"user":{"displayName":"Himadyuti Bhanja","photoUrl":"","userId":"06992247206842326199"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["%%bash\n","pip install regex requests"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (2.21.0)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests) (2020.4.5.1)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests) (2.8)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests) (3.0.4)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"G_uIrd038DHL","colab_type":"code","outputId":"71145530-262a-44ca-9fcb-38ad5b8c697b","executionInfo":{"status":"ok","timestamp":1588261731300,"user_tz":-330,"elapsed":921,"user":{"displayName":"Himadyuti Bhanja","photoUrl":"","userId":"06992247206842326199"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1qX3j5k08Egk","colab_type":"code","colab":{}},"source":["from torch.utils.data import Dataset\n","import numpy as np \n","import torch\n","import os\n","import json\n","import pickle\n","import random\n","\n","def pad_tensor(vec, pad, dim):\n","    pad_size = list(vec.shape)\n","    pad_size[dim] = pad - vec.size(dim)\n","    return torch.cat([vec, torch.zeros(*pad_size)], dim=dim)\t\n","\n","use_cuda = torch.cuda.is_available()\n","device = torch.device('cuda:0' if use_cuda else 'cpu')\n","\n","#roberta = torch.hub.load('pytorch/fairseq', 'roberta.large.mnli')\n","#roberta.eval()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vKF6GtwD8HtV","colab_type":"code","outputId":"145d36f2-7ac4-4d71-8273-947b418e89a2","executionInfo":{"status":"ok","timestamp":1588131516004,"user_tz":-330,"elapsed":6942,"user":{"displayName":"Himadyuti Bhanja","photoUrl":"","userId":"06992247206842326199"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["!pip install py-rouge\n","import rouge\n","\n","def prepare_results(p, r, f):\n","    return '\\t{}:\\t{}: {:5.2f}\\t{}: {:5.2f}\\t{}: {:5.2f}'.format(metric, 'P', 100.0 * p, 'R', 100.0 * r, 'F1', 100.0 * f)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting py-rouge\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/1d/0bdbaf559fb7afe32308ebc84a2028600988212d7eb7fb9f69c4e829e4a0/py_rouge-1.1-py3-none-any.whl (56kB)\n","\r\u001b[K     |█████▊                          | 10kB 18.2MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 30kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 2.0MB/s \n","\u001b[?25hInstalling collected packages: py-rouge\n","Successfully installed py-rouge-1.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"y4j9owuk8WRR","colab_type":"code","outputId":"b506a4af-201c-4897-a542-0dad25e3216b","executionInfo":{"status":"ok","timestamp":1588250509519,"user_tz":-330,"elapsed":3457,"user":{"displayName":"Himadyuti Bhanja","photoUrl":"","userId":"06992247206842326199"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["import pickle\n","cnn_dataset_path = '/content/drive/My Drive/Colab Notebooks/summarization_data/cnn_dataset_test.pkl'\n","dm_dataset_path = '/content/drive/My Drive/Colab Notebooks/summarization_data/dailymail_dataset_test.pkl'\n","file = open(cnn_dataset_path, \"rb\")\n","\n","stories_cnn = pickle.load(file)\n","print('Loaded Stories %d' % len(stories_cnn))\n","\n","file = open(dm_dataset_path, \"rb\")\n","\n","stories_dm = pickle.load(file)\n","print('Loaded Stories %d' % len(stories_dm))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Loaded Stories 186\n","Loaded Stories 436\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RB7prDLH8Y8F","colab_type":"code","colab":{}},"source":["contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n","\n","                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n","\n","                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n","\n","                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n","\n","                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n","\n","                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n","\n","                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n","\n","                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n","\n","                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n","\n","                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n","\n","                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n","\n","                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n","\n","                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n","\n","                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n","\n","                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n","\n","                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n","\n","                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n","\n","                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n","\n","                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n","\n","                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n","\n","                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n","\n","                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n","\n","                           \"you're\": \"you are\", \"you've\": \"you have\"}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5XkeoYy28cNg","colab_type":"code","outputId":"ccb7f6ab-4ef9-47c3-8411-acecefc27e65","executionInfo":{"status":"ok","timestamp":1588250518130,"user_tz":-330,"elapsed":1880,"user":{"displayName":"Himadyuti Bhanja","photoUrl":"","userId":"06992247206842326199"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["import re\n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","\n","stop_words = stopwords.words('english')\n","\n","def preprocess(text):\n","    #text = text.lower() # lowercase\n","    text = text.split() # convert have'nt -> have not\n","    for i in range(len(text)):\n","        word = text[i]\n","        if word in contraction_mapping:\n","            text[i] = contraction_mapping[word]\n","    text = \" \".join(text)\n","    #text = text.split()\n","    '''newtext = []\n","    for word in text:\n","        if word not in stop_words:\n","            newtext.append(word)\n","    text = \" \".join(newtext)'''\n","    #text = text.replace(\"'s\",'') # convert your's -> your\n","    text = re.sub(r'\\(.*\\)','',text) # remove (words)\n","    #text = re.sub(r'[^a-zA-Z0-9. ]','',text) # remove punctuations\n","    #text = re.sub(r'\\.',' . ',text)\n","    text = text.replace(\"-\", '')\n","    return text"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9oz5IGbL8fdc","colab_type":"code","outputId":"dad15234-422c-4f45-d124-251fe2245feb","executionInfo":{"status":"ok","timestamp":1588250523640,"user_tz":-330,"elapsed":2051,"user":{"displayName":"Himadyuti Bhanja","photoUrl":"","userId":"06992247206842326199"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["import nltk.data\n","nltk.download('punkt')\n","tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n","\n","text = []\n","summary = []\n","for story in stories_cnn:\n","  x = \". \".join(story['highlights'])\n","  x += '.'\n","  summary.append(x)\n","  x = preprocess(story['story'])\n","  x = tokenizer.tokenize(x)\n","  text.append(x)\n","for story in stories_dm:\n","  x = \". \".join(story['highlights'])\n","  x += '.'\n","  summary.append(x)\n","  x = preprocess(story['story'])\n","  x = tokenizer.tokenize(x)\n","  text.append(x)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BwSL9Uop8jDe","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","\n","class SelfAttention(nn.Module):\n","\tdef __init__(self, batch_size=32, hidden_size=200, embedding_length=768):\n","\t\tsuper(SelfAttention, self).__init__()\n","\n","\t\t\"\"\"\n","\t\tArguments\n","\t\t---------\n","\t\tbatch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n","\t\toutput_size : 2 = (pos, neg)\n","\t\thidden_sie : Size of the hidden_state of the LSTM\n","\t\tvocab_size : Size of the vocabulary containing unique words\n","\t\tembedding_length : Embeddding dimension of GloVe word embeddings\n","\t\tweights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table \n","\t\t\n","\t\t--------\n","\t\t\n","\t\t\"\"\"\n","\n","\t\tself.batch_size = batch_size\n","\t\tself.hidden_size = hidden_size\n","\n","\t\tself.bilstm = nn.LSTM(embedding_length, hidden_size, 2, dropout=0.2, bidirectional=True)\n","\t\t# We will use da = 350, r = 30 & penalization_coeff = 1 as per given in the self-attention original ICLR paper\n","\t\tself.W_s1 = nn.Linear(2*hidden_size, 350)\n","\t\tself.W_s2 = nn.Linear(350, 30)\n","\t\tself.fc_layer = nn.Linear(30*2*hidden_size, 2000)\n","\n","\tdef attention_net(self, lstm_output):\n","\n","\t\t\"\"\"\n","\t\tNow we will use self attention mechanism to produce a matrix embedding of the input sentence in which every row represents an\n","\t\tencoding of the inout sentence but giving an attention to a specific part of the sentence. We will use 30 such embedding of \n","\t\tthe input sentence and then finally we will concatenate all the 30 sentence embedding vectors and connect it to a fully \n","\t\tconnected layer of size 2000 which will be connected to the output layer of size 2 returning logits for our two classes i.e., \n","\t\tpos & neg.\n","\t\tArguments\n","\t\t---------\n","\t\tlstm_output = A tensor containing hidden states corresponding to each time step of the LSTM network.\n","\t\t---------\n","\t\tReturns : Final Attention weight matrix for all the 30 different sentence embedding in which each of 30 embeddings give\n","\t\t\t\t  attention to different parts of the input sentence.\n","\t\tTensor size : lstm_output.size() = (batch_size, num_seq, 2*hidden_size)\n","\t\t\t\t\t  attn_weight_matrix.size() = (batch_size, 30, num_seq)\n","\t\t\"\"\"\n","\t\tattn_weight_matrix = self.W_s2(F.tanh(self.W_s1(lstm_output)))\n","\t\tattn_weight_matrix = attn_weight_matrix.permute(0, 2, 1)\n","\t\tattn_weight_matrix = F.softmax(attn_weight_matrix, dim=2)\n","\n","\t\treturn attn_weight_matrix\n","\n","\tdef forward(self, input, batch_size=None):\n","\n","\t\t\"\"\" \n","\t\tParameters\n","\t\t----------\n","\t\tinput_sentence: input_sentence of shape = (batch_size, num_sequences)\n","\t\tbatch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n","\t\t\n","\t\tReturns\n","\t\t-------\n","\t\tOutput of the linear layer containing logits for pos & neg class.\n","\t\t\n","\t\t\"\"\"\n","\n","\t\t#input = self.word_embeddings(input_sentences)\n","\t\tinput = input.permute(1, 0, 2)\n","\t\tif batch_size is None:\n","\t\t\th_0 = Variable(torch.zeros(4, self.batch_size, self.hidden_size).cuda())\n","\t\t\tc_0 = Variable(torch.zeros(4, self.batch_size, self.hidden_size).cuda())\n","\t\telse:\n","\t\t\th_0 = Variable(torch.zeros(4, batch_size, self.hidden_size).cuda())\n","\t\t\tc_0 = Variable(torch.zeros(4, batch_size, self.hidden_size).cuda())\n","\n","\t\toutput, (h_n, c_n) = self.bilstm(input, (h_0, c_0))\n","\t\toutput = output.permute(1, 0, 2)\n","\t\t# output.size() = (batch_size, num_seq, 2*hidden_size)\n","\t\t# h_n.size() = (1, batch_size, hidden_size)\n","\t\t# c_n.size() = (1, batch_size, hidden_size)\n","\t\tattn_weight_matrix = self.attention_net(output)\n","\t\t# attn_weight_matrix.size() = (batch_size, r, num_seq)\n","\t\t# output.size() = (batch_size, num_seq, 2*hidden_size)\n","\t\thidden_matrix = torch.bmm(attn_weight_matrix, output)\n","\t\t# hidden_matrix.size() = (batch_size, r, 2*hidden_size)\n","\t\t# Let's now concatenate the hidden_matrix and connect it to the fully connected layer.\n","\t\tfc_out = self.fc_layer(hidden_matrix.view(-1, hidden_matrix.size()[1]*hidden_matrix.size()[2]))\n","\t\t# logits.size() = (batch_size, output_size)\n","\n","\t\treturn fc_out\n","\n","class LSTM1(nn.Module):\n","\tdef __init__(self, batch_size=256, hidden_size=200, embedding_length=768, num_labels=3):\n","\t\tsuper(LSTM1, self).__init__()\n","\n","\t\tself.premise_net = SelfAttention(batch_size, hidden_size, embedding_length)\n","\t\tself.hypothesis_net = SelfAttention(batch_size, hidden_size, embedding_length)\n","\t\tself.fc = nn.Linear(4000, 3)\n","\n","\tdef forward(self, premise, hypothesis):\n","\t\trep1 = self.premise_net(premise)\n","\t\trep2 = self.hypothesis_net(hypothesis)\n","\t\trep = torch.cat([rep1, rep2], 1)\n","\t\treturn self.fc(rep)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lhh2dBWM8pdq","colab_type":"code","outputId":"9df7176c-3c48-4dc8-af95-847ac9557d05","executionInfo":{"status":"ok","timestamp":1588250540913,"user_tz":-330,"elapsed":6692,"user":{"displayName":"Himadyuti Bhanja","photoUrl":"","userId":"06992247206842326199"}},"colab":{"base_uri":"https://localhost:8080/","height":411}},"source":["!pip3 install pytorch_pretrained_bert"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting pytorch_pretrained_bert\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n","\r\u001b[K     |██▋                             | 10kB 25.3MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 11.6MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 10.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 9.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 9.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 9.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 9.4MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.3)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.12.46)\n","Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.5.0+cu101)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.38.0)\n","Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.21.0)\n","Requirement already satisfied: botocore<1.16.0,>=1.15.46 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.15.46)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.9.5)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.16.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.4.5.1)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.8)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.46->boto3->pytorch_pretrained_bert) (2.8.1)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.46->boto3->pytorch_pretrained_bert) (0.15.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.16.0,>=1.15.46->boto3->pytorch_pretrained_bert) (1.12.0)\n","Installing collected packages: pytorch-pretrained-bert\n","Successfully installed pytorch-pretrained-bert-0.6.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_gESsvOG8sNT","colab_type":"code","outputId":"f102ed1a-6da1-42d7-f36c-37d7ded66017","executionInfo":{"status":"ok","timestamp":1588250586187,"user_tz":-330,"elapsed":42992,"user":{"displayName":"Himadyuti Bhanja","photoUrl":"","userId":"06992247206842326199"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["from pytorch_pretrained_bert import BertTokenizer, BertModel\n","bert_model = BertModel.from_pretrained('bert-base-uncased')\n","bert_model = bert_model.cuda()\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","lstm = LSTM1(batch_size = 1).to(device)\n","lstm.load_state_dict(torch.load(\"/content/drive/My Drive/IG wala/checkpoints/networkbertlstm1_train_epoch_27.ckpt\"))\n","    \n","def pad_tensor(vec, pad, dim):\n","    pad_size = list(vec.shape)\n","    pad_size[dim] = pad - vec.size(dim)\n","    return torch.cat([vec, torch.zeros(*pad_size)], dim=dim)\t\n","\n","def predict(premise, hypothesis):\n","    marked_text = \"[CLS] \" + premise + \" [SEP]\"\n","    tokenized_text = tokenizer.tokenize(marked_text)\n","    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n","    segments_ids = [1] * len(tokenized_text)\n","    tokens_tensor = torch.tensor([indexed_tokens])\n","    segments_tensors = torch.tensor([segments_ids])\n","    bert_model.eval()\n","    with torch.no_grad():\n","      encoded_layers, _ = bert_model(tokens_tensor.to(device), segments_tensors.to(device))\n","      embedding = encoded_layers[11][0]\n","      premise = np.array(embedding.cpu())\n","    marked_text = \"[CLS] \" + hypothesis + \" [SEP]\"\n","    tokenized_text = tokenizer.tokenize(marked_text)\n","    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n","    segments_ids = [1] * len(tokenized_text)\n","    tokens_tensor = torch.tensor([indexed_tokens])\n","    segments_tensors = torch.tensor([segments_ids])\n","    bert_model.eval()\n","    with torch.no_grad():\n","      encoded_layers, _ = bert_model(tokens_tensor.to(device), segments_tensors.to(device))\n","      embedding = encoded_layers[11][0]\n","      hypothesis = np.array(embedding.cpu())\n","    premise = torch.tensor(premise).type(torch.FloatTensor)\n","    hypothesis = torch.tensor(hypothesis).type(torch.FloatTensor)\n","    premise = pad_tensor(premise, 250, 0)\n","    hypothesis = pad_tensor(hypothesis, 250, 0)\n","    premise = premise.unsqueeze(0)\n","    hypothesis = hypothesis.unsqueeze(0)\n","    y_pred = lstm(premise.to(device), hypothesis.to(device))\n","    return y_pred\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["100%|██████████| 407873900/407873900 [00:14<00:00, 28603631.97B/s]\n","100%|██████████| 231508/231508 [00:00<00:00, 908798.58B/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"PQtKzdcz8u11","colab_type":"code","outputId":"58772de2-3f53-41bc-fcf1-c1c0f4abda9a","executionInfo":{"status":"ok","timestamp":1588267219760,"user_tz":-330,"elapsed":5382602,"user":{"displayName":"Himadyuti Bhanja","photoUrl":"","userId":"06992247206842326199"}},"colab":{"base_uri":"https://localhost:8080/","height":646}},"source":["gen_summary = []\n","for it in range(500, 622):\n","  print('\\r', it, end = '')\n","  graph = {}\n","  sentences = text[it]\n","  for sentence1 in sentences:\n","   adj_list = []\n","   for sentence2 in sentences:\n","     if sentence1 == sentence2:\n","       continue\n","     s = []\n","     s.append(sentence1)\n","     s.append(sentence2)\n","     if len(s) != 2:\n","       continue\n","     try:\n","      with torch.no_grad():\n","         prediction = predict(s[0], s[1])\n","         _, prediction = prediction.max(1)\n","         #print(prediction)\n","         if prediction == 2:\n","          adj_list.append(sentence2)\n","     except Exception as e:\n","       print(e)\n","     if len(adj_list) > 0:\n","       graph[sentence1] = adj_list\n","  result = []\n","  while len(graph)>0:\n","    s1 = next(iter(graph))\n","    if(len(s1)<=5):\n","      del graph[s1]\n","      continue\n","    result.append(s1)\n","    f = 0\n","    i = 0\n","    s2 = graph[s1][0]\n","    while(len(s2)<=5):\n","      try:\n","        del graph[s2]\n","      except:\n","        pass\n","      i = i+1\n","      try:\n","        s2 = graph[s1][i]\n","      except:\n","        f = 1\n","        break\n","    if f == 0:\n","      result.append(s2)\n","    del graph[s1]\n","    try:\n","      del graph[s2]\n","    except:\n","      pass\n","  result = list(dict.fromkeys(result))\n","  result = [x[:len(x)-1] + \".\" for x in result]\n","  result = \" \".join(result)\n","  gen_summary.append(result)\n","  if it == 403:\n","    print(result)\n","\n","file = open('/content/drive/My Drive/Colab Notebooks/pickle/gen_summaries_cnndm_5.pkl','wb+')\n","pickle.dump((gen_summary), file)\n","file.close()"],"execution_count":18,"outputs":[{"output_type":"stream","text":["\r 500"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1558: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":[" 549Trying to create tensor with negative dimension -44: [-44, 768]\n","Trying to create tensor with negative dimension -44: [-44, 768]\n","Trying to create tensor with negative dimension -44: [-44, 768]\n","Trying to create tensor with negative dimension -44: [-44, 768]\n","Trying to create tensor with negative dimension -44: [-44, 768]\n","Trying to create tensor with negative dimension -44: [-44, 768]\n","Trying to create tensor with negative dimension -44: [-44, 768]\n","Trying to create tensor with negative dimension -44: [-44, 768]\n","Trying to create tensor with negative dimension -44: [-44, 768]\n","Trying to create tensor with negative dimension -44: [-44, 768]\n"," 602Trying to create tensor with negative dimension -24: [-24, 768]\n","Trying to create tensor with negative dimension -24: [-24, 768]\n","Trying to create tensor with negative dimension -24: [-24, 768]\n","Trying to create tensor with negative dimension -24: [-24, 768]\n","Trying to create tensor with negative dimension -24: [-24, 768]\n","Trying to create tensor with negative dimension -24: [-24, 768]\n","Trying to create tensor with negative dimension -24: [-24, 768]\n","Trying to create tensor with negative dimension -24: [-24, 768]\n","Trying to create tensor with negative dimension -24: [-24, 768]\n","Trying to create tensor with negative dimension -24: [-24, 768]\n","Trying to create tensor with negative dimension -24: [-24, 768]\n","Trying to create tensor with negative dimension -24: [-24, 768]\n","Trying to create tensor with negative dimension -24: [-24, 768]\n","Trying to create tensor with negative dimension -24: [-24, 768]\n","Trying to create tensor with negative dimension -24: [-24, 768]\n","Trying to create tensor with negative dimension -24: [-24, 768]\n","Trying to create tensor with negative dimension -24: [-24, 768]\n","Trying to create tensor with negative dimension -24: [-24, 768]\n","Trying to create tensor with negative dimension -24: [-24, 768]\n","Trying to create tensor with negative dimension -24: [-24, 768]\n","Trying to create tensor with negative dimension -24: [-24, 768]\n","Trying to create tensor with negative dimension -24: [-24, 768]\n","Trying to create tensor with negative dimension -24: [-24, 768]\n","Trying to create tensor with negative dimension -24: [-24, 768]\n"," 621"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NN4erCiqc6KZ","colab_type":"code","outputId":"c14001aa-51b3-4398-b011-2d75e9bb4825","executionInfo":{"status":"ok","timestamp":1588256320131,"user_tz":-330,"elapsed":1306,"user":{"displayName":"Himadyuti Bhanja","photoUrl":"","userId":"06992247206842326199"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["print(gen_summary[82])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["By Amanda Williams It could have been the story of the year  'The Queen has tested positive for morphine'. Unfortunately, however, it was little more than a welltimed screen grab. A picture appearing to show the BBC report that the Queen has tested positive for the drug morphine  and not her horse  is provoking much hilarity on Twitter. The screen grab  taken at an opportune moment as the rolling news text scrolled across the bottom of the screen  omitted the crucial first half of the sentence. The BBC appeared to report that the Queen has tested positive for the drug morphine when this screen grab was taken  missing out key words Estimate owned by the Queen tested positive for morphine, a prohibited substance, Buckingham Palace said In its entirety, it would have read: 'A racehorse owned by the Queen has tested positive for morphine.. It was revealed last night that Estimate, one of Her Majesty's prize racehorses, had tested positive for the banned drug. BBC bosses seem to have seen the funny side of the joke  saying that usually viewers watch the news in whole, and not frame by frame. A spokesman said: 'The ticker on the BBC News Channel correctly showed the whole headline \"A racehorse owned by the Queen has tested positive for morphine\" as it scrolled along the screen. Trained by leading flat racing trainer Sir Michael Stoute, Estimate won the Ascot Gold Cup last year  the first royal horse to win the prestigious race. A screen grab is doing the rounds on Twitter  causing much hilarity She came second in this year’s Gold Cup last month, when the test is believed to have occurred. The revelation came five days after the British Horseracing Authority  to take the Gold Cup on Ladies Day at Royal Ascot this year Estimate, ridden by Ryan Moore, in action during of The Sagaro Stakes at Ascot in May 2013 If Estimate did test positive after this year’s Gold Cup, the horse could be stripped of her second place and the Queen would forfeit the £80,625 prize. When Estimate won last year’s Gold Cup, the Queen cheered her to victory and then joined jockey Ryan Moore and trainer Sir Michael for the presentation – which had to be hastily rearranged as she had been due to hand over the trophy. The Duke of York stepped in to make the presentation to his mother, who also won £155,960 in prize money.\n"],"name":"stdout"}]}]}