{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Evaluation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uxLums65cwv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "fecd67c1-8e04-4fb6-aa71-4e7fb6474574"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVEY0NSdMMfC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "outputId": "3cc4d263-9817-417b-c3c2-567b98550a20"
      },
      "source": [
        "!pip3 install pytorch_pretrained_bert"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch_pretrained_bert in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.5.0+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.14.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.4.5.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.16.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.2 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.17.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.2->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.2->boto3->pytorch_pretrained_bert) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.18.0,>=1.17.2->boto3->pytorch_pretrained_bert) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-8aWIoiMQTY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "503f474b-a19d-4c42-c075-ae71487ca592"
      },
      "source": [
        "!pip install allennlp==v0.9.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting allennlp==v0.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/bb/041115d8bad1447080e5d1e30097c95e4b66e36074277afce8620a61cee3/allennlp-0.9.0-py3-none-any.whl (7.6MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6MB 1.3MB/s \n",
            "\u001b[?25hCollecting gevent>=1.3.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/05/18/3932900a42d7010cc63529fc5cb7a5a20fb61878d1721d0d4387567d5973/gevent-20.6.2-cp36-cp36m-manylinux2010_x86_64.whl (5.3MB)\n",
            "\u001b[K     |████████████████████████████████| 5.3MB 57.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from allennlp==v0.9.0) (0.5.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp==v0.9.0) (3.2.5)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from allennlp==v0.9.0) (3.2.1)\n",
            "Collecting pytorch-pretrained-bert>=0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 57.9MB/s \n",
            "\u001b[?25hCollecting responses>=0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/06/4c/4cc4a9ad8a592692d579660d9040d5693b30cb066e4ba4159aa4a5e5fb65/responses-0.10.15-py2.py3-none-any.whl\n",
            "Collecting jsonnet>=0.10.0; sys_platform != \"win32\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/70/ed1ba808a87d896b9f4d25400dda54e089ca7a97e87cee620b3744997c89/jsonnet-0.16.0.tar.gz (256kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 53.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from allennlp==v0.9.0) (1.5.0+cu101)\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp==v0.9.0) (4.41.1)\n",
            "Collecting overrides\n",
            "  Downloading https://files.pythonhosted.org/packages/ff/b1/10f69c00947518e6676bbd43e739733048de64b8dd998e9c2d5a71f44c5d/overrides-3.1.0.tar.gz\n",
            "Requirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from allennlp==v0.9.0) (1.1.2)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from allennlp==v0.9.0) (1.14.2)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp==v0.9.0) (3.6.4)\n",
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 51.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp==v0.9.0) (0.22.2.post1)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp==v0.9.0) (2.23.0)\n",
            "Collecting flaky\n",
            "  Downloading https://files.pythonhosted.org/packages/fe/12/0f169abf1aa07c7edef4855cca53703d2e6b7ecbded7829588ac7e7e3424/flaky-3.6.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: sqlparse>=0.2.4 in /usr/local/lib/python3.6/dist-packages (from allennlp==v0.9.0) (0.3.1)\n",
            "Collecting flask-cors>=3.0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/78/38/e68b11daa5d613e3a91e4bf3da76c94ac9ee0d9cd515af9c1ab80d36f709/Flask_Cors-3.0.8-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from allennlp==v0.9.0) (1.18.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.6/dist-packages (from allennlp==v0.9.0) (2018.9)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp==v0.9.0) (1.4.1)\n",
            "Collecting jsonpickle\n",
            "  Downloading https://files.pythonhosted.org/packages/af/ca/4fee219cc4113a5635e348ad951cf8a2e47fed2e3342312493f5b73d0007/jsonpickle-1.4.1-py2.py3-none-any.whl\n",
            "Collecting numpydoc>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/43/2402fd1f63992a52f88e3b169d59674617013bf7f1ad0cd7d842eafd0c58/numpydoc-1.0.0-py3-none-any.whl (47kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.1MB/s \n",
            "\u001b[?25hCollecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/d8/5e877ac5e827eaa41a7ea8c0dc1d3042e05d7e337604dc2aedb854e7b500/ftfy-5.7.tar.gz (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.1MB/s \n",
            "\u001b[?25hCollecting parsimonious>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/fc/067a3f89869a41009e1a7cdfb14725f8ddd246f30f63c645e8ef8a1c56f4/parsimonious-0.8.1.tar.gz (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.9MB/s \n",
            "\u001b[?25hCollecting spacy<2.2,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/5b/e07dd3bf104237bce4b398558b104c8e500333d6f30eabe3fa9685356b7d/spacy-2.1.9-cp36-cp36m-manylinux1_x86_64.whl (30.8MB)\n",
            "\u001b[K     |████████████████████████████████| 30.9MB 100kB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp==v0.9.0) (2.10.0)\n",
            "Collecting conllu==1.3.1\n",
            "  Downloading https://files.pythonhosted.org/packages/ae/54/b0ae1199f3d01666821b028cd967f7c0ac527ab162af433d3da69242cea2/conllu-1.3.1-py2.py3-none-any.whl\n",
            "Collecting word2number>=1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\n",
            "Collecting tensorboardX>=1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/f1/5843425495765c8c2dd0784a851a93ef204d314fc87bcc2bbb9f662a3ad1/tensorboardX-2.0-py2.py3-none-any.whl (195kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 51.5MB/s \n",
            "\u001b[?25hCollecting pytorch-transformers==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/89/ad0d6bb932d0a51793eaabcf1617a36ff530dc9ab9e38f765a35dc293306/pytorch_transformers-1.1.0-py3-none-any.whl (158kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 22.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from gevent>=1.3.6->allennlp==v0.9.0) (47.1.1)\n",
            "Collecting zope.interface\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/57/33/565274c28a11af60b7cfc0519d46bde4125fcd7d32ebc0a81b480d0e8da6/zope.interface-5.1.0-cp36-cp36m-manylinux2010_x86_64.whl (234kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 52.6MB/s \n",
            "\u001b[?25hCollecting greenlet>=0.4.16; platform_python_implementation == \"CPython\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/a4/0d8685c98986326534b0753a8b92b3082bc9df42b348bc50d6c69839c9f9/greenlet-0.4.16-cp36-cp36m-manylinux1_x86_64.whl (44kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.1MB/s \n",
            "\u001b[?25hCollecting zope.event\n",
            "  Downloading https://files.pythonhosted.org/packages/c5/96/361edb421a077a4c208b4a5c212737d78ae03ce67fbbcd01621c49f332d1/zope.event-4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->allennlp==v0.9.0) (1.12.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp==v0.9.0) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp==v0.9.0) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp==v0.9.0) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp==v0.9.0) (0.10.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert>=0.6.0->allennlp==v0.9.0) (2019.12.20)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.2.0->allennlp==v0.9.0) (0.16.0)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp==v0.9.0) (7.1.2)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp==v0.9.0) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp==v0.9.0) (1.0.1)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp==v0.9.0) (2.11.2)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.2 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp==v0.9.0) (1.17.2)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp==v0.9.0) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp==v0.9.0) (0.10.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp==v0.9.0) (8.4.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp==v0.9.0) (19.3.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp==v0.9.0) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp==v0.9.0) (1.8.1)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp==v0.9.0) (1.4.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->allennlp==v0.9.0) (0.15.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp==v0.9.0) (2020.4.5.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp==v0.9.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp==v0.9.0) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp==v0.9.0) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from jsonpickle->allennlp==v0.9.0) (1.6.1)\n",
            "Requirement already satisfied: sphinx>=1.6.5 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp==v0.9.0) (1.8.5)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->allennlp==v0.9.0) (0.2.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp==v0.9.0) (2.0.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp==v0.9.0) (1.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp==v0.9.0) (1.0.2)\n",
            "Collecting plac<1.0.0,>=0.9.6\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl\n",
            "Collecting thinc<7.1.0,>=7.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/a5/9ace20422e7bb1bdcad31832ea85c52a09900cd4a7ce711246bfb92206ba/thinc-7.0.8-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 51.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp==v0.9.0) (0.6.0)\n",
            "Collecting blis<0.3.0,>=0.2.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/46/b1d0bb71d308e820ed30316c5f0a017cb5ef5f4324bcbc7da3cf9d3b075c/blis-0.2.4-cp36-cp36m-manylinux1_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 44.7MB/s \n",
            "\u001b[?25hCollecting preshed<2.1.0,>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/93/f222fb957764a283203525ef20e62008675fd0a14ffff8cc1b1490147c63/preshed-2.0.1-cp36-cp36m-manylinux1_x86_64.whl (83kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 12.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp==v0.9.0) (3.10.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 47.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->flask>=1.0.2->allennlp==v0.9.0) (1.1.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.2->boto3->allennlp==v0.9.0) (0.15.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->jsonpickle->allennlp==v0.9.0) (3.1.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==v0.9.0) (2.1.3)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==v0.9.0) (0.7.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==v0.9.0) (20.4)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==v0.9.0) (1.2.2)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==v0.9.0) (2.0.0)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==v0.9.0) (2.8.0)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==v0.9.0) (1.2.0)\n",
            "Building wheels for collected packages: jsonnet, overrides, ftfy, parsimonious, word2number\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.16.0-cp36-cp36m-linux_x86_64.whl size=3321574 sha256=ff37533ef38610c1dc6d3d153ae7ea6b5c503fd802370fbad6ee49716d936cd1\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/a9/43/bc5e0463deeec89dfca928a2a64595f1bdb520c891f6fbd09c\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-cp36-none-any.whl size=10174 sha256=f689bcc631cea982b43ee69c90d253d8d7940ed811ff5d8ec4431e153280e655\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/24/13/6ef8600e6f147c95e595f1289a86a3cc82ed65df57582c65a9\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.7-cp36-none-any.whl size=44593 sha256=71d3ddcce6035fe173c51ab34026d37819d68679d8b30b78e63378217ab1691f\n",
            "  Stored in directory: /root/.cache/pip/wheels/8e/da/59/6c8925d571aacade638a0f515960c21c0887af1bfe31908fbf\n",
            "  Building wheel for parsimonious (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parsimonious: filename=parsimonious-0.8.1-cp36-none-any.whl size=42712 sha256=362e235a31f269b2bc382cead2ad314d92e81844c9464b39e36a2013b2a6190f\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/8d/e7/a0e74217da5caeb3c1c7689639b6d28ddbf9985b840bc96a9a\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-cp36-none-any.whl size=5587 sha256=944104ec47036f4384208aa37503ebb4d76d23a86ccbc730de9b422bbdc924d5\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/2f/53/5f5c1d275492f2fce1cdab9a9bb12d49286dead829a4078e0e\n",
            "Successfully built jsonnet overrides ftfy parsimonious word2number\n",
            "\u001b[31mERROR: en-core-web-sm 2.2.5 has requirement spacy>=2.2.2, but you'll have spacy 2.1.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: zope.interface, greenlet, zope.event, gevent, pytorch-pretrained-bert, responses, jsonnet, overrides, unidecode, flaky, flask-cors, jsonpickle, numpydoc, ftfy, parsimonious, plac, blis, preshed, thinc, spacy, conllu, word2number, tensorboardX, sentencepiece, pytorch-transformers, allennlp\n",
            "  Found existing installation: plac 1.1.3\n",
            "    Uninstalling plac-1.1.3:\n",
            "      Successfully uninstalled plac-1.1.3\n",
            "  Found existing installation: blis 0.4.1\n",
            "    Uninstalling blis-0.4.1:\n",
            "      Successfully uninstalled blis-0.4.1\n",
            "  Found existing installation: preshed 3.0.2\n",
            "    Uninstalling preshed-3.0.2:\n",
            "      Successfully uninstalled preshed-3.0.2\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed allennlp-0.9.0 blis-0.2.4 conllu-1.3.1 flaky-3.6.1 flask-cors-3.0.8 ftfy-5.7 gevent-20.6.2 greenlet-0.4.16 jsonnet-0.16.0 jsonpickle-1.4.1 numpydoc-1.0.0 overrides-3.1.0 parsimonious-0.8.1 plac-0.9.6 preshed-2.0.1 pytorch-pretrained-bert-0.6.2 pytorch-transformers-1.1.0 responses-0.10.15 sentencepiece-0.1.91 spacy-2.1.9 tensorboardX-2.0 thinc-7.0.8 unidecode-1.1.1 word2number-1.1 zope.event-4.4 zope.interface-5.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgCzX6ZAJWWC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "outputId": "9fd258f7-6c3f-4bc3-c770-4ab7020dd184"
      },
      "source": [
        "!pip install -U sentence-transformers"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/46/b7d6c37d92d1bd65319220beabe4df845434930e3f30e42d3cfaecb74dc4/sentence-transformers-0.2.6.1.tar.gz (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 4.4MB/s \n",
            "\u001b[?25hCollecting transformers>=2.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl (674kB)\n",
            "\u001b[K     |████████████████████████████████| 675kB 22.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.5.0+cu101)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 44.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=2.8.0->sentence-transformers) (0.7)\n",
            "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.8.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from transformers>=2.8.0->sentence-transformers) (20.4)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from transformers>=2.8.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers>=2.8.0->sentence-transformers) (0.1.91)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 47.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.8.0->sentence-transformers) (3.0.12)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.1->sentence-transformers) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers) (0.15.1)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers>=2.8.0->sentence-transformers) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.8.0->sentence-transformers) (2020.4.5.2)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.8.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.8.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.8.0->sentence-transformers) (2.9)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.8.0->sentence-transformers) (7.1.2)\n",
            "Building wheels for collected packages: sentence-transformers, sacremoses\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-0.2.6.1-cp36-none-any.whl size=74031 sha256=b38f59e60b5976abe3e3f634f89313fb6266554a9004abba45e32e73c7886192\n",
            "  Stored in directory: /root/.cache/pip/wheels/d7/fa/17/2b081a8cd8b0a86753fb0e9826b3cc19f0207062c0b2da7008\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=aaafb7c93805d2dd09ba0f2b66ba0d49d7354e6d17153dd2677405a0e19e548b\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sentence-transformers sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers, sentence-transformers\n",
            "Successfully installed sacremoses-0.0.43 sentence-transformers-0.2.6.1 tokenizers-0.7.0 transformers-2.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxHwPiMzvedn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "class WiCNet(nn.Module):\n",
        "\tdef __init__(self, batch_size=32, embedding_length=1024, num_labels=2):\n",
        "\t\tsuper(WiCNet, self).__init__()\n",
        "\n",
        "\t\tself.wicnet = nn.Sequential(nn.Linear(2*embedding_length, 1000),\n",
        "\t\t\tnn.Softmax(),\n",
        "      nn.Dropout(),\n",
        "\t\t\tnn.Linear(1000, num_labels),\n",
        "\t\t\tnn.Softmax()\n",
        "\t\t\t)\n",
        "\n",
        "\tdef forward(self, word1, word2):\n",
        "\t\tword_concat = torch.cat([word1, word2], 1)\n",
        "\t\treturn self.wicnet(word_concat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_op6GHZywpQZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "outputId": "953cb295-9ec2-4ac9-edf8-64eb034426b5"
      },
      "source": [
        "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
        "bert_model = BertModel.from_pretrained('bert-large-uncased')\n",
        "bert_model = bert_model.cuda()\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "bert_base = BertModel.from_pretrained('bert-base-uncased')\n",
        "bert_base = bert_model.cuda()\n",
        "tokenizer_base = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1248501532/1248501532 [00:41<00:00, 30197010.45B/s]\n",
            "100%|██████████| 231508/231508 [00:00<00:00, 615448.29B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJVDOLC0GYsC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device('cuda:0' if use_cuda else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2yEpZ-bGdDn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "outputId": "43bcda96-2166-471e-f679-14455a0a41f8"
      },
      "source": [
        "import numpy as np \n",
        "import torch\n",
        "from allennlp.commands.elmo import ElmoEmbedder\n",
        "elmo = ElmoEmbedder(cuda_device=0) "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 336/336 [00:00<00:00, 662569.88B/s]\n",
            "100%|██████████| 374434792/374434792 [00:24<00:00, 15476409.11B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KuuGbTGIFPf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "\tdef __init__(self, batch_size=32, hidden_size=200, embedding_length=768):\n",
        "\t\tsuper(SelfAttention, self).__init__()\n",
        "\n",
        "\t\t\"\"\"\n",
        "\t\tArguments\n",
        "\t\t---------\n",
        "\t\tbatch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
        "\t\toutput_size : 2 = (pos, neg)\n",
        "\t\thidden_sie : Size of the hidden_state of the LSTM\n",
        "\t\tvocab_size : Size of the vocabulary containing unique words\n",
        "\t\tembedding_length : Embeddding dimension of GloVe word embeddings\n",
        "\t\tweights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table \n",
        "\t\t\n",
        "\t\t--------\n",
        "\t\t\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\tself.batch_size = batch_size\n",
        "\t\tself.hidden_size = hidden_size\n",
        "\n",
        "\t\tself.bilstm = nn.LSTM(embedding_length, hidden_size, 2, dropout=0.2, bidirectional=True)\n",
        "\t\t# We will use da = 350, r = 30 & penalization_coeff = 1 as per given in the self-attention original ICLR paper\n",
        "\t\tself.W_s1 = nn.Linear(2*hidden_size, 350)\n",
        "\t\tself.W_s2 = nn.Linear(350, 30)\n",
        "\t\tself.fc_layer = nn.Linear(30*2*hidden_size, 2000)\n",
        "\n",
        "\tdef attention_net(self, lstm_output):\n",
        "\n",
        "\t\t\"\"\"\n",
        "\t\tNow we will use self attention mechanism to produce a matrix embedding of the input sentence in which every row represents an\n",
        "\t\tencoding of the inout sentence but giving an attention to a specific part of the sentence. We will use 30 such embedding of \n",
        "\t\tthe input sentence and then finally we will concatenate all the 30 sentence embedding vectors and connect it to a fully \n",
        "\t\tconnected layer of size 2000 which will be connected to the output layer of size 2 returning logits for our two classes i.e., \n",
        "\t\tpos & neg.\n",
        "\t\tArguments\n",
        "\t\t---------\n",
        "\t\tlstm_output = A tensor containing hidden states corresponding to each time step of the LSTM network.\n",
        "\t\t---------\n",
        "\t\tReturns : Final Attention weight matrix for all the 30 different sentence embedding in which each of 30 embeddings give\n",
        "\t\t\t\t  attention to different parts of the input sentence.\n",
        "\t\tTensor size : lstm_output.size() = (batch_size, num_seq, 2*hidden_size)\n",
        "\t\t\t\t\t  attn_weight_matrix.size() = (batch_size, 30, num_seq)\n",
        "\t\t\"\"\"\n",
        "\t\tattn_weight_matrix = self.W_s2(F.tanh(self.W_s1(lstm_output)))\n",
        "\t\tattn_weight_matrix = attn_weight_matrix.permute(0, 2, 1)\n",
        "\t\tattn_weight_matrix = F.softmax(attn_weight_matrix, dim=2)\n",
        "\n",
        "\t\treturn attn_weight_matrix\n",
        "\n",
        "\tdef forward(self, input, batch_size=None):\n",
        "\n",
        "\t\t\"\"\" \n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tinput_sentence: input_sentence of shape = (batch_size, num_sequences)\n",
        "\t\tbatch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n",
        "\t\t\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\tOutput of the linear layer containing logits for pos & neg class.\n",
        "\t\t\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\t#input = self.word_embeddings(input_sentences)\n",
        "\t\tinput = input.permute(1, 0, 2)\n",
        "\t\tif batch_size is None:\n",
        "\t\t\th_0 = Variable(torch.zeros(4, self.batch_size, self.hidden_size).cuda())\n",
        "\t\t\tc_0 = Variable(torch.zeros(4, self.batch_size, self.hidden_size).cuda())\n",
        "\t\telse:\n",
        "\t\t\th_0 = Variable(torch.zeros(4, batch_size, self.hidden_size).cuda())\n",
        "\t\t\tc_0 = Variable(torch.zeros(4, batch_size, self.hidden_size).cuda())\n",
        "\n",
        "\t\toutput, (h_n, c_n) = self.bilstm(input, (h_0, c_0))\n",
        "\t\toutput = output.permute(1, 0, 2)\n",
        "\t\t# output.size() = (batch_size, num_seq, 2*hidden_size)\n",
        "\t\t# h_n.size() = (1, batch_size, hidden_size)\n",
        "\t\t# c_n.size() = (1, batch_size, hidden_size)\n",
        "\t\tattn_weight_matrix = self.attention_net(output)\n",
        "\t\t# attn_weight_matrix.size() = (batch_size, r, num_seq)\n",
        "\t\t# output.size() = (batch_size, num_seq, 2*hidden_size)\n",
        "\t\thidden_matrix = torch.bmm(attn_weight_matrix, output)\n",
        "\t\t# hidden_matrix.size() = (batch_size, r, 2*hidden_size)\n",
        "\t\t# Let's now concatenate the hidden_matrix and connect it to the fully connected layer.\n",
        "\t\tfc_out = self.fc_layer(hidden_matrix.view(-1, hidden_matrix.size()[1]*hidden_matrix.size()[2]))\n",
        "\t\t# logits.size() = (batch_size, output_size)\n",
        "\n",
        "\t\treturn fc_out\n",
        "\n",
        "class LSTM1(nn.Module):\n",
        "\tdef __init__(self, batch_size=256, hidden_size=200, embedding_length=768, num_labels=3):\n",
        "\t\tsuper(LSTM1, self).__init__()\n",
        "\n",
        "\t\tself.premise_net = SelfAttention(batch_size, hidden_size, embedding_length)\n",
        "\t\tself.hypothesis_net = SelfAttention(batch_size, hidden_size, embedding_length)\n",
        "\t\tself.fc = nn.Linear(4000, 3)\n",
        "\n",
        "\tdef forward(self, premise, hypothesis):\n",
        "\t\trep1 = self.premise_net(premise)\n",
        "\t\trep2 = self.hypothesis_net(hypothesis)\n",
        "\t\trep = torch.cat([rep1, rep2], 1)\n",
        "\t\treturn self.fc(rep)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiQs8h-TIK36",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "def pad_tensor(vec, pad, dim):\n",
        "    pad_size = list(vec.shape)\n",
        "    pad_size[dim] = pad - vec.size(dim)\n",
        "    return torch.cat([vec, torch.zeros(*pad_size)], dim=dim)\t\n",
        "\n",
        "def predict_bert(premise, hypothesis):\n",
        "    lstm = LSTM1(batch_size = 1).to(device)\n",
        "    lstm.load_state_dict(torch.load(\"/content/drive/My Drive/IG wala/checkpoints/networkbertlstm1_train_epoch_27.ckpt\"))\n",
        "\n",
        "    marked_text = \"[CLS] \" + premise + \" [SEP]\"\n",
        "    tokenized_text = tokenizer_base.tokenize(marked_text)\n",
        "    indexed_tokens = tokenizer_base.convert_tokens_to_ids(tokenized_text)\n",
        "    segments_ids = [1] * len(tokenized_text)\n",
        "    tokens_tensor = torch.tensor([indexed_tokens])\n",
        "    segments_tensors = torch.tensor([segments_ids])\n",
        "    bert_base.eval()\n",
        "    with torch.no_grad():\n",
        "      encoded_layers, _ = bert_bese(tokens_tensor.to(device), segments_tensors.to(device))\n",
        "      embedding = encoded_layers[11][0]\n",
        "      premise = np.array(embedding.cpu())\n",
        "    marked_text = \"[CLS] \" + hypothesis + \" [SEP]\"\n",
        "    tokenized_text = tokenizer_base.tokenize(marked_text)\n",
        "    indexed_tokens = tokenizer_base.convert_tokens_to_ids(tokenized_text)\n",
        "    segments_ids = [1] * len(tokenized_text)\n",
        "    tokens_tensor = torch.tensor([indexed_tokens])\n",
        "    segments_tensors = torch.tensor([segments_ids])\n",
        "    bert_base.eval()\n",
        "    with torch.no_grad():\n",
        "      encoded_layers, _ = bert_base(tokens_tensor.to(device), segments_tensors.to(device))\n",
        "      embedding = encoded_layers[11][0]\n",
        "      hypothesis = np.array(embedding.cpu())\n",
        "    premise = torch.tensor(premise).type(torch.FloatTensor)\n",
        "    hypothesis = torch.tensor(hypothesis).type(torch.FloatTensor)\n",
        "    premise = pad_tensor(premise, 250, 0)\n",
        "    hypothesis = pad_tensor(hypothesis, 250, 0)\n",
        "    premise = premise.unsqueeze(0)\n",
        "    hypothesis = hypothesis.unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "      y_pred = lstm(premise.to(device), hypothesis.to(device))\n",
        "    return y_pred\n",
        "\n",
        "#def predict_elmo(premise, hypothesis):\n",
        "#  lstm = LSTM1(batch_size = 1).to(device)\n",
        "#  lstm.load_state_dict(torch.load(\"/content/drive/My Drive/IG wala/checkpoints/networkbertlstm1_train_epoch_27.ckpt\"))\n",
        "\n",
        "#  embedding = elmo.embed_sentence(premise.split())\n",
        "#  premise_embedding = np.average(embedding, axis = 0)\n",
        "  \n",
        "#  embedding = elmo.embed_sentence(hypothesis.split())\n",
        "#  hypothesis_embedding = np.average(embedding, axis = 0)\n",
        "\n",
        "#  premise = torch.tensor(premise_embedding).type(torch.FloatTensor)\n",
        "#  hypothesis = torch.tensor(hypothesis_embedding).type(torch.FloatTensor)\n",
        "#  premise = pad_tensor(premise, 250, 0)\n",
        "#  hypothesis = pad_tensor(hypothesis, 250, 0)\n",
        "#  premise = premise.unsqueeze(0)\n",
        "#  hypothesis = hypothesis.unsqueeze(0)\n",
        "#  y_pred = lstm(premise.to(device), hypothesis.to(device))\n",
        "#  return y_pred\n",
        "\n",
        "def predict_roberta(premise, hypothesis):\n",
        "  roberta = torch.hub.load('pytorch/fairseq', 'roberta.large.mnli')\n",
        "  roberta.eval()\n",
        "  with torch.no_grad():\n",
        "    tokens = roberta.encode(premise, hypothesis)\n",
        "    prediction = roberta.predict('mnli', tokens).argmax().item()\n",
        "  return prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbrmJWNCfNbH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "\n",
        "                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "\n",
        "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "\n",
        "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
        "\n",
        "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "\n",
        "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
        "\n",
        "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
        "\n",
        "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
        "\n",
        "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "\n",
        "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
        "\n",
        "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
        "\n",
        "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "\n",
        "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "\n",
        "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
        "\n",
        "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
        "\n",
        "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
        "\n",
        "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "\n",
        "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "\n",
        "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
        "\n",
        "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
        "\n",
        "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "\n",
        "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
        "\n",
        "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PF-1s7-FXQmd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        },
        "outputId": "76efaa5b-de65-4f6f-fea1-cf7c0448629a"
      },
      "source": [
        "import nltk.data\n",
        "nltk.download('punkt')\n",
        "tokenizer1 = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "def preprocess(text):\n",
        "    #text = text.lower() # lowercase\n",
        "    text = text.split() # convert have'nt -> have not\n",
        "    for i in range(len(text)):\n",
        "        word = text[i]\n",
        "        if word in contraction_mapping:\n",
        "            text[i] = contraction_mapping[word]\n",
        "    text = \" \".join(text)\n",
        "    #text = text.split()\n",
        "    '''newtext = []\n",
        "    for word in text:\n",
        "        if word not in stop_words:\n",
        "            newtext.append(word)\n",
        "    text = \" \".join(newtext)'''\n",
        "    #text = text.replace(\"'s\",'') # convert your's -> your\n",
        "    text = re.sub(r'\\(.*\\)','',text) # remove (words)\n",
        "    #text = re.sub(r'[^a-zA-Z0-9. ]','',text) # remove punctuations\n",
        "    #text = re.sub(r'\\.',' . ',text)\n",
        "    text = text.replace(\"-\", '')\n",
        "    return text\n",
        "\n",
        "def sort_by_values_len(dict):\n",
        "  dict_len= {key: len(value) for key, value in dict.items()}\n",
        "  import operator\n",
        "  sorted_key_list = sorted(dict_len.items(), key=operator.itemgetter(1), reverse=True)\n",
        "  sorted_dict = [{item[0]: dict[item [0]]} for item in sorted_key_list]\n",
        "  return sorted_dict\n",
        "\n",
        "def dominating_set(st, sentences, graph):\n",
        "  dom_set = []\n",
        "  for token in st:\n",
        "    dom_set.extend(graph[token])\n",
        "  if(dom_set.sort() == sentences.sort()):\n",
        "    return True\n",
        "  else:\n",
        "    return False"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rzQHWkmv_rH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        },
        "outputId": "e109bace-bde7-41f1-999b-720bd0ae3fc1"
      },
      "source": [
        "print(\"Enter first sentence\")\n",
        "sentence1 = \"The bank won't accept cash on Saturday.\" #input()\n",
        "print(\"Enter second sentence\")\n",
        "sentence2 = \"The river overflowed it's bank.\" #input()\n",
        "print(\"Enter index of required word in sentence 1\")\n",
        "sentence1_index = 1 #int(input())\n",
        "print(\"Enter index of required word in sentence 2\")\n",
        "sentence2_index = 4 #int(input())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter first sentence\n",
            "Enter second sentence\n",
            "Enter index of required word in sentence 1\n",
            "Enter index of required word in sentence 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAkJIcrpwxjY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        },
        "outputId": "80235f38-b548-47e4-a290-acc40769a7fb"
      },
      "source": [
        "marked_text = \"[CLS] \" + sentence1 + \" [SEP]\"\n",
        "tokenized_text = tokenizer.tokenize(marked_text)\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "segments_ids = [1] * len(tokenized_text)\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "segments_tensors = torch.tensor([segments_ids])\n",
        "bert_model.eval()\n",
        "with torch.no_grad():\n",
        "\tencoded_layers, _ = bert_model(tokens_tensor.to(device), segments_tensors.to(device))\n",
        "\tembedding = encoded_layers[11][0]\n",
        "\tembedding1 = embedding[sentence1_index]\n",
        "\n",
        "marked_text = \"[CLS] \" + sentence2 + \" [SEP]\"\n",
        "tokenized_text = tokenizer.tokenize(marked_text)\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "segments_ids = [1] * len(tokenized_text)\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "segments_tensors = torch.tensor([segments_ids])\n",
        "bert_model.eval()\n",
        "with torch.no_grad():\n",
        "\tencoded_layers, _ = bert_model(tokens_tensor.to(device), segments_tensors.to(device))\n",
        "\tembedding = encoded_layers[11][0]\n",
        "\tembedding2 = embedding[sentence2_index]\n",
        "\n",
        "embedding1, embedding2 = torch.tensor(embedding1).type(torch.FloatTensor), torch.tensor(embedding2).type(torch.FloatTensor)\n",
        "embedding1 = embedding1.unsqueeze(0)\n",
        "embedding2 = embedding2.unsqueeze(0)\n",
        "dnn = WiCNet(batch_size = 1).to(device)\n",
        "dnn.load_state_dict(torch.load(\"/content/drive/My Drive/Colab Notebooks/checkpoints/networkwicbert_train_epoch_59.ckpt\"))\n",
        "\n",
        "y_pred = dnn(embedding1.to(device), embedding2.to(device))\n",
        "_, predicted = y_pred.max(1)\n",
        "if(predicted==0):\n",
        "  print(\"False\")\n",
        "else:\n",
        "  print(\"True\")\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:100: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xq2kYH3y3mMd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "68b1d178-0234-43aa-dc57-97b98d7d3d35"
      },
      "source": [
        "embedding = elmo.embed_sentence(sentence1.split())\n",
        "embedding = np.average(embedding, axis = 0)\n",
        "embedding1 = embedding[sentence1_index]\n",
        "\n",
        "embedding = elmo.embed_sentence(sentence2.split())\n",
        "embedding = np.average(embedding, axis = 0)\n",
        "embedding2 = embedding[sentence1_index]\n",
        "embedding1, embedding2 = torch.tensor(embedding1).type(torch.FloatTensor), torch.tensor(embedding2).type(torch.FloatTensor)\n",
        "embedding1 = embedding1.unsqueeze(0)\n",
        "embedding2 = embedding2.unsqueeze(0)\n",
        "\n",
        "dnn = WiCNet(batch_size = 1).to(device)\n",
        "dnn.load_state_dict(torch.load(\"/content/drive/My Drive/Colab Notebooks/checkpoints/networkwicelmo_train_epoch_57.ckpt\"))\n",
        "\n",
        "y_pred = dnn(embedding1.to(device), embedding2.to(device))\n",
        "_, predicted = y_pred.max(1)\n",
        "if(predicted==0):\n",
        "  print(\"False\")\n",
        "else:\n",
        "  print(\"True\")\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:100: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26YCotD8H5HU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "premise = \"Two blond women are hugging one another.\" #input(\"Enter the premise: \")\n",
        "hypothesis = \"There are women showing affection.\" #input(\"Enter the hypothesis: \")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fL4Ffice_U32",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "1c7531f7-1cc0-4150-8d9c-c875cde7ccd1"
      },
      "source": [
        "print(\"_______BERT_________\")\n",
        "prediction = predict_bert(premise, hypothesis)\n",
        "_, prediction = prediction.max(1)\n",
        "if(prediction==2):\n",
        "  print(\"Entailment\")\n",
        "elif(prediction==1):\n",
        "  print(\"Contradiction\")\n",
        "else:\n",
        "  print(\"Neutral\")\n",
        "\n",
        "print(\"_______RoBERTa________\")\n",
        "prediction = predict_roberta(premise, hypothesis)\n",
        "if(prediction==2):\n",
        "  print(\"Entailment\")\n",
        "elif(prediction==1):\n",
        "  print(\"Neutral\")\n",
        "else:\n",
        "  print(\"Contradiction\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_______BERT_________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1558: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_fairseq_master\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Entailment\n",
            "_______RoBERTa________\n",
            "Entailment\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doGDvUYUWp-a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "outputId": "97c1fdda-fa0e-4016-ae0c-6bc861154d22"
      },
      "source": [
        "text = input(\"Enter the text to be summarized: \")"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the text to be summarized: Extractive text summarization involves the selection of phrases and sentences from the source document to make up the new summary. Techniques involve ranking the relevance of phrases in order to choose only those most relevant to the meaning of the source.  Abstractive text summarization involves generating entirely new phrases and sentences to capture the meaning of the source document. This is a more challenging approach, but is also the approach ultimately used by humans. Classical methods operate by selecting and compressing content from the source document.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ko2ZSCPYXxu1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = preprocess(text)\n",
        "text = tokenizer1.tokenize(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmTuIInAYVKJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        },
        "outputId": "aa3735fa-6e71-4239-8166-abaa9128b68b"
      },
      "source": [
        "graph = {}\n",
        "sentences = text\n",
        "#print(sentences)\n",
        "for sentence1 in sentences:\n",
        "   adj_list = []\n",
        "   for sentence2 in sentences:\n",
        "     if sentence1 == sentence2:\n",
        "       continue\n",
        "     s = []\n",
        "     s.append(sentence1)\n",
        "     s.append(sentence2)\n",
        "     if len(s) != 2:\n",
        "       continue\n",
        "     try:\n",
        "      with torch.no_grad():\n",
        "         prediction = predict_bert(s[0], s[1])\n",
        "         _, prediction = prediction.max(1)\n",
        "         #print(prediction)\n",
        "         if prediction == 2:\n",
        "          #print(\"yes\")\n",
        "          adj_list.append(sentence2)\n",
        "     except Exception as e:\n",
        "       print(e)\n",
        "     if len(adj_list) > 0:\n",
        "       graph[sentence1] = adj_list\n",
        "result = []\n",
        "while len(graph)>0:\n",
        "  s1 = next(iter(graph))\n",
        "  if(len(s1)<=5):\n",
        "    del graph[s1]\n",
        "    continue\n",
        "  result.append(s1)\n",
        "  f = 0\n",
        "  i = 0\n",
        "  s2 = graph[s1][0]\n",
        "  while(len(s2)<=5):\n",
        "    try:\n",
        "      del graph[s2]\n",
        "    except:\n",
        "      pass\n",
        "    i = i+1\n",
        "    try:\n",
        "      s2 = graph[s1][i]\n",
        "    except:\n",
        "      f = 1\n",
        "      break\n",
        "  if f == 0:\n",
        "    result.append(s2)\n",
        "  del graph[s1]\n",
        "  try:\n",
        "    del graph[s2]\n",
        "  except:\n",
        "    pass\n",
        "result = list(dict.fromkeys(result))\n",
        "result = [x[:len(x)-1] + \".\" for x in result]\n",
        "result = \" \".join(result)\n",
        "print(result)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Extractive text summarization involves the selection of phrases and sentences from the source document to make up the new summary.', 'Techniques involve ranking the relevance of phrases in order to choose only those most relevant to the meaning of the source.', 'Abstractive text summarization involves generating entirely new phrases and sentences to capture the meaning of the source document.', 'This is a more challenging approach, but is also the approach ultimately used by humans.', 'Classical methods operate by selecting and compressing content from the source document.']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1558: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extractive text summarization involves the selection of phrases and sentences from the source document to make up the new summary. Techniques involve ranking the relevance of phrases in order to choose only those most relevant to the meaning of the source. Abstractive text summarization involves generating entirely new phrases and sentences to capture the meaning of the source document. This is a more challenging approach, but is also the approach ultimately used by humans. Classical methods operate by selecting and compressing content from the source document.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nUB5POKY8Rw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "5396574c-e4a3-4234-a171-85874f303160"
      },
      "source": [
        "graph = {}\n",
        "sentences = text\n",
        "#print(sentences)\n",
        "for sentence1 in sentences:\n",
        "   adj_list = []\n",
        "   for sentence2 in sentences:\n",
        "     if sentence1 == sentence2:\n",
        "       continue\n",
        "     s = []\n",
        "     s.append(sentence1)\n",
        "     s.append(sentence2)\n",
        "     if len(s) != 2:\n",
        "       continue\n",
        "     try:\n",
        "      with torch.no_grad():\n",
        "         prediction = predict_bert(s[0], s[1])\n",
        "         _, prediction = prediction.max(1)\n",
        "         #print(prediction)\n",
        "         if prediction == 2:\n",
        "          #print(\"yes\")\n",
        "          adj_list.append(sentence2)\n",
        "     except Exception as e:\n",
        "       print(e)\n",
        "     if len(adj_list) > 0:\n",
        "       graph[sentence1] = adj_list\n",
        "graph_list = sort_by_values_len(graph)\n",
        "result = []\n",
        "if len(graph)>0 :\n",
        "  h = 0\n",
        "  result.append(list(graph_list[h].keys())[0])\n",
        "  while(dominating_set(result, sentences, graph) == 0):\n",
        "    h = h + 1\n",
        "    result.append(list(graph_list[h].keys())[0])\n",
        "result = [x[:len(x)-1] + \".\" for x in result]\n",
        "result = \"\".join(result)\n",
        "print(result)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1558: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Abstractive text summarization involves generating entirely new phrases and sentences to capture the meaning of the source document.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}