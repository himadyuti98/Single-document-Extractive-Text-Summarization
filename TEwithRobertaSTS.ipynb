{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TEwithRobertaSTS.ipynb","provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyMXO9M8iKn25xKw0yjxBfHS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"YruE10dYUNQw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":850},"outputId":"fd4fdc19-866a-4efa-e83e-c7023b5e6956","executionInfo":{"status":"ok","timestamp":1583559367620,"user_tz":-330,"elapsed":11245,"user":{"displayName":"Himadyuti Bhanja","photoUrl":"","userId":"06992247206842326199"}}},"source":["!pip install -U sentence-transformers"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting sentence-transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c9/91/c85ddef872d5bb39949386930c1f834ac382e145fcd30155b09d6fb65c5a/sentence-transformers-0.2.5.tar.gz (49kB)\n","\u001b[K     |████████████████████████████████| 51kB 1.7MB/s \n","\u001b[?25hCollecting transformers==2.3.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/10/aeefced99c8a59d828a92cc11d213e2743212d3641c87c82d61b035a7d5c/transformers-2.3.0-py3-none-any.whl (447kB)\n","\u001b[K     |████████████████████████████████| 450kB 8.0MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (4.28.1)\n","Requirement already satisfied, skipping upgrade: torch>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.4.0)\n","Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.17.5)\n","Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (0.22.1)\n","Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.4.1)\n","Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (3.2.5)\n","Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0->sentence-transformers) (2.21.0)\n","Requirement already satisfied, skipping upgrade: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0->sentence-transformers) (1.11.15)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n","\u001b[K     |████████████████████████████████| 870kB 43.0MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0->sentence-transformers) (2019.12.20)\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n","\u001b[K     |████████████████████████████████| 1.0MB 43.4MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers) (0.14.1)\n","Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers) (1.12.0)\n","Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->sentence-transformers) (1.24.3)\n","Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->sentence-transformers) (2019.11.28)\n","Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->sentence-transformers) (2.8)\n","Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->sentence-transformers) (3.0.4)\n","Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0->sentence-transformers) (0.9.4)\n","Requirement already satisfied, skipping upgrade: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0->sentence-transformers) (1.14.15)\n","Requirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0->sentence-transformers) (0.3.3)\n","Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.3.0->sentence-transformers) (7.0)\n","Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers==2.3.0->sentence-transformers) (0.15.2)\n","Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers==2.3.0->sentence-transformers) (2.6.1)\n","Building wheels for collected packages: sentence-transformers, sacremoses\n","  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sentence-transformers: filename=sentence_transformers-0.2.5-cp36-none-any.whl size=64942 sha256=96e4735c578083d835dc9ab7f19bc1e19b287dae4c36dc3960142789d7f938d2\n","  Stored in directory: /root/.cache/pip/wheels/b4/ce/39/5bbda8ac34eb52df8c6531382ca077773fbfcbfb6386e5d66c\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=733ec084b044d1d25ad24eecceaded67af7e4291498918393c8930fc8c36d884\n","  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n","Successfully built sentence-transformers sacremoses\n","Installing collected packages: sacremoses, sentencepiece, transformers, sentence-transformers\n","Successfully installed sacremoses-0.0.38 sentence-transformers-0.2.5 sentencepiece-0.1.85 transformers-2.3.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MtFPsscEUa-j","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":134},"outputId":"3a252f31-75b5-4c42-fd85-69ac2504a009","executionInfo":{"status":"ok","timestamp":1583559392400,"user_tz":-330,"elapsed":22679,"user":{"displayName":"Himadyuti Bhanja","photoUrl":"","userId":"06992247206842326199"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"B7rbQ_sUUl4c","colab_type":"code","colab":{}},"source":["from torch.utils.data import Dataset\n","import numpy as np \n","import torch\n","import os\n","import json\n","import pickle\n","import random\n","\n","def pad_tensor(vec, pad, dim):\n","    pad_size = list(vec.shape)\n","    pad_size[dim] = pad - vec.size(dim)\n","    return torch.cat([vec, torch.zeros(*pad_size)], dim=dim)\t\n","\n","use_cuda = torch.cuda.is_available()\n","device = torch.device('cuda:0' if use_cuda else 'cpu')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PmNhvFGzUsBI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"279a1ddd-74b2-4360-f048-cc6aacd0762b","executionInfo":{"status":"ok","timestamp":1583559448088,"user_tz":-330,"elapsed":45647,"user":{"displayName":"Himadyuti Bhanja","photoUrl":"","userId":"06992247206842326199"}}},"source":["from sentence_transformers import SentenceTransformer\n","model = SentenceTransformer('roberta-base-nli-stsb-mean-tokens')"],"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["100%|██████████| 459M/459M [00:23<00:00, 19.3MB/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"tdA-1jM0V6Z1","colab_type":"code","colab":{}},"source":["class SentenceDatasetRoberta(Dataset):\n","\tdef __init__(self, load=False, test=False):\n","\t\tsuper(SentenceDatasetRoberta, self).__init__()\n","\n","\n","\t\tself.premise = []\n","\t\tself.hypothesis = []\n","\t\tself.label = []\n","\t\tself.premise_test = []\n","\t\tself.hypothesis_test = []\n","\t\tself.label_test = []\n","\t\tself.max_len = 0\n","\t\tself.test = test\n","\t\tself.batch_size = 32\n","\n","\t\tlab = {\n","\t\t\t\t\"neutral\" : 0,\n","\t\t\t\t\"contradiction\" : 1,\n","\t\t\t\t\"entailment\" : 2\n","\t\t}\n","\n","\n","\t\tif(load):\n","\t\t\ttraindata = '/content/drive/My Drive/Colab Notebooks/mnli_1.0/multinli_1.0_train.jsonl'\n","\t\t\ttestdata = '/content/drive/My Drive/Colab Notebooks/mnli_1.0/multinli_1.0_dev_matched.jsonl'\n","\n","\t\t\tif(test==False):\n","\t\t\t\twith open(traindata, \"r\") as f:\n","\t\t\t\t\ti = 0\n","\t\t\t\t\tfor line in f:\n","\t\t\t\t\t\t\n","\t\t\t\t\t\tjsondata = json.loads(line)\n","\n","\t\t\t\t\t\tlabl = jsondata[\"gold_label\"]\n","\n","\t\t\t\t\t\tif labl == \"-\":\n","\t\t\t\t\t\t\tcontinue\n","\t\t\t\t\t\ti = i+1\n","\t\t\t\t\t\tself.label.append(lab[labl])\n","\t\t\t\t\t\t\n","\t\t\t\t\t\tsentence = jsondata[\"sentence1\"]\n","\t\t\t\t\t\tself.premise.append(sentence)\n","\t\t\t\t\t\tsentence = sentence.split(' ')\n","\t\t\t\t\t\tif(len(sentence) > self.max_len):\n","\t\t\t\t\t\t\tself.max_len = len(sentence)\n","\t\t\t\t\t\t\n","\t\t\t\t\t\tsentence = jsondata[\"sentence2\"]\n","\t\t\t\t\t\tself.hypothesis.append(sentence)\n","\t\t\t\t\t\tsentence = sentence.split(' ')\n","\t\t\t\t\t\tif(len(sentence) > self.max_len):\n","\t\t\t\t\t\t\tself.max_len = len(sentence)\n","\t\t\t\t\t\t\n","\t\t\t\t\t\t\n","\t\t\t\t\t\t\n","\t\t\t\t\t\tif(i%1000==0):\n","\t\t\t\t\t\t\tprint(i)\n","\t\t\t\t\t'''nums = range(len(self.premise))\n","\t\t\t\t\tnumlist = random.choices(nums, k=40000)\n","\t\t\t\t\tp = []\n","\t\t\t\t\th = []\n","\t\t\t\t\tl = []\n","\t\t\t\t\tfor i in numlist:\n","\t\t\t\t\t\tp.append(self.premise[i])\n","\t\t\t\t\t\th.append(self.hypothesis[i])\n","\t\t\t\t\t\tl.append(self.label[i])\n","\n","\t\t\t\t\tself.premise = model.encode(p)\n","\t\t\t\t\tself.hypothesis = model.encode(h)\n","\t\t\t\t\tself.label = l'''\n","\t\t\t\t\tself.premise = model.encode(self.premise)\n","\t\t\t\t\tself.hypothesis = model.encode(self.hypothesis)\n","\t\t\t\t\tfile = open('/content/drive/My Drive/Colab Notebooks/pickle/sentences_roberta_train_full.dat','wb+')\n","\t\t\t\t\tpickle.dump((self.premise, self.hypothesis, self.label), file)\n","\t\t\t\t\tprint(\"done on training data \", self.max_len, i)\n","\t\t\t\t\n","\n","\t\t\telse:\n","\t\t\t\twith open(testdata, \"r\") as f:\n","\t\t\t\t\ti = 0;\n","\t\t\t\t\tfor line in f:\n","\t\t\t\t\t\t\n","\t\t\t\t\t\tjsondata = json.loads(line)\n","\n","\t\t\t\t\t\tlabl = jsondata[\"gold_label\"]\n","\n","\t\t\t\t\t\tif labl == \"-\":\n","\t\t\t\t\t\t\tcontinue\n","\t\t\t\t\t\ti=i+1\n","\t\t\t\t\t\tself.label_test.append(lab[labl])\n","\t\t\t\t\t\t\n","\t\t\t\t\t\tsentence = jsondata[\"sentence1\"]\n","\t\t\t\t\t\tself.premise_test.append(sentence)\n","\t\t\t\t\t\tsentence = sentence.split(' ')\n","\t\t\t\t\t\tif(len(sentence) > self.max_len):\n","\t\t\t\t\t\t\tself.max_len = len(sentence)\n","\t\t\t\t\t\t\n","\t\t\t\t\t\tsentence = jsondata[\"sentence2\"]\n","\t\t\t\t\t\tself.hypothesis_test.append(sentence)\n","\t\t\t\t\t\tsentence = sentence.split(' ')\n","\t\t\t\t\t\tif(len(sentence) > self.max_len):\n","\t\t\t\t\t\t\tself.max_len = len(sentence)\n","\t\t\t\t\t\t\n","\t\t\t\n","\t\t\t\t\t\tif(i%1000==0):\n","\t\t\t\t\t\t\tprint(i)\n","\t\t\t\t\t\t\t\n","\t\t\t\t\tself.premise_test = model.encode(self.premise_test)\n","\t\t\t\t\tself.hypothesis_test = model.encode(self.hypothesis_test)\n","\t\t\t\t\tfile = open('/content/drive/My Drive/Colab Notebooks/pickle/sentences_roberta_test.dat','wb+')\n","\t\t\t\t\tpickle.dump((self.premise_test, self.hypothesis_test, self.label_test), file)\n","\t\t\t\t\tprint(\"done on test data \", self.max_len, i)\n","\t\telse:\n","\t\t\tif(self.test):\n","\t\t\t\tfile = open('/content/drive/My Drive/Colab Notebooks/pickle/sentences_roberta_test.dat', 'rb+')\n","\t\t\t\tself.premise_test, self.hypothesis_test, self.label_test = pickle.load(file)\n","\t\t\t\tfile.close()\n","\t\t\telse:\n","\t\t\t\tfile = open('/content/drive/My Drive/Colab Notebooks/pickle/sentences_roberta_train_full.dat', 'rb+')\n","\t\t\t\tself.premise, self.hypothesis, self.label = pickle.load(file)\n","\t\t\t\tfile.close()\n","\n","\tdef __len__(self):\n","\t\tif(self.test):\n","\t\t\treturn 9815\n","\t\telse:\n","\t\t\treturn 392700\n","\n","\tdef __getitem__(self, idx):\t\n","\t\t\n","\t\tif(self.test):\n","\t\t\tpremise, hypothesis, label = self.premise_test[idx], self.hypothesis_test[idx], self.label_test[idx] #torch.tensor(self.premise_test[idx]).type(torch.FloatTensor), torch.tensor(self.hypothesis_test[idx]).type(torch.FloatTensor), torch.tensor(self.label_test[idx]).type(torch.LongTensor)\n","\t\telse:\n","\t\t\tpremise, hypothesis, label = self.premise[idx], self.hypothesis[idx], self.label[idx] #torch.tensor(self.premise[idx]).type(torch.FloatTensor), torch.tensor(self.hypothesis[idx]).type(torch.FloatTensor), torch.tensor(self.label[idx]).type(torch.LongTensor)\n","\t\t'''p = []\n","\t\tp.append(premise)\n","\t\tpremise = model.encode(p)[0]\n","\t\th = []\n","\t\th.append(hypothesis)\n","\t\thypothesis = model.encode(h)[0]'''\n","\n","\t\tpremise, hypothesis, label = torch.tensor(premise).type(torch.FloatTensor), torch.tensor(hypothesis).type(torch.FloatTensor), torch.tensor(label).type(torch.LongTensor)\n","\t\t\t\t\t\t\n","\t\t#premise = pad_tensor(premise, 80, 0)\n","\t\t#hypothesis = pad_tensor(hypothesis, 80, 0)\n","\t\treturn premise, hypothesis, label\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bXcmAB31WBLo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"d983f2d5-2e78-4c46-8c5c-f0592377597e","executionInfo":{"status":"ok","timestamp":1583561404037,"user_tz":-330,"elapsed":1844732,"user":{"displayName":"Himadyuti Bhanja","photoUrl":"","userId":"06992247206842326199"}}},"source":["#SentenceDatasetRoberta(load=True, test=True)\n","SentenceDatasetRoberta(load=True, test=False)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["1000\n","2000\n","3000\n","4000\n","5000\n","6000\n","7000\n","8000\n","9000\n","10000\n","11000\n","12000\n","13000\n","14000\n","15000\n","16000\n","17000\n","18000\n","19000\n","20000\n","21000\n","22000\n","23000\n","24000\n","25000\n","26000\n","27000\n","28000\n","29000\n","30000\n","31000\n","32000\n","33000\n","34000\n","35000\n","36000\n","37000\n","38000\n","39000\n","40000\n","41000\n","42000\n","43000\n","44000\n","45000\n","46000\n","47000\n","48000\n","49000\n","50000\n","51000\n","52000\n","53000\n","54000\n","55000\n","56000\n","57000\n","58000\n","59000\n","60000\n","61000\n","62000\n","63000\n","64000\n","65000\n","66000\n","67000\n","68000\n","69000\n","70000\n","71000\n","72000\n","73000\n","74000\n","75000\n","76000\n","77000\n","78000\n","79000\n","80000\n","81000\n","82000\n","83000\n","84000\n","85000\n","86000\n","87000\n","88000\n","89000\n","90000\n","91000\n","92000\n","93000\n","94000\n","95000\n","96000\n","97000\n","98000\n","99000\n","100000\n","101000\n","102000\n","103000\n","104000\n","105000\n","106000\n","107000\n","108000\n","109000\n","110000\n","111000\n","112000\n","113000\n","114000\n","115000\n","116000\n","117000\n","118000\n","119000\n","120000\n","121000\n","122000\n","123000\n","124000\n","125000\n","126000\n","127000\n","128000\n","129000\n","130000\n","131000\n","132000\n","133000\n","134000\n","135000\n","136000\n","137000\n","138000\n","139000\n","140000\n","141000\n","142000\n","143000\n","144000\n","145000\n","146000\n","147000\n","148000\n","149000\n","150000\n","151000\n","152000\n","153000\n","154000\n","155000\n","156000\n","157000\n","158000\n","159000\n","160000\n","161000\n","162000\n","163000\n","164000\n","165000\n","166000\n","167000\n","168000\n","169000\n","170000\n","171000\n","172000\n","173000\n","174000\n","175000\n","176000\n","177000\n","178000\n","179000\n","180000\n","181000\n","182000\n","183000\n","184000\n","185000\n","186000\n","187000\n","188000\n","189000\n","190000\n","191000\n","192000\n","193000\n","194000\n","195000\n","196000\n","197000\n","198000\n","199000\n","200000\n","201000\n","202000\n","203000\n","204000\n","205000\n","206000\n","207000\n","208000\n","209000\n","210000\n","211000\n","212000\n","213000\n","214000\n","215000\n","216000\n","217000\n","218000\n","219000\n","220000\n","221000\n","222000\n","223000\n","224000\n","225000\n","226000\n","227000\n","228000\n","229000\n","230000\n","231000\n","232000\n","233000\n","234000\n","235000\n","236000\n","237000\n","238000\n","239000\n","240000\n","241000\n","242000\n","243000\n","244000\n","245000\n","246000\n","247000\n","248000\n","249000\n","250000\n","251000\n","252000\n","253000\n","254000\n","255000\n","256000\n","257000\n","258000\n","259000\n","260000\n","261000\n","262000\n","263000\n","264000\n","265000\n","266000\n","267000\n","268000\n","269000\n","270000\n","271000\n","272000\n","273000\n","274000\n","275000\n","276000\n","277000\n","278000\n","279000\n","280000\n","281000\n","282000\n","283000\n","284000\n","285000\n","286000\n","287000\n","288000\n","289000\n","290000\n","291000\n","292000\n","293000\n","294000\n","295000\n","296000\n","297000\n","298000\n","299000\n","300000\n","301000\n","302000\n","303000\n","304000\n","305000\n","306000\n","307000\n","308000\n","309000\n","310000\n","311000\n","312000\n","313000\n","314000\n","315000\n","316000\n","317000\n","318000\n","319000\n","320000\n","321000\n","322000\n","323000\n","324000\n","325000\n","326000\n","327000\n","328000\n","329000\n","330000\n","331000\n","332000\n","333000\n","334000\n","335000\n","336000\n","337000\n","338000\n","339000\n","340000\n","341000\n","342000\n","343000\n","344000\n","345000\n","346000\n","347000\n","348000\n","349000\n","350000\n","351000\n","352000\n","353000\n","354000\n","355000\n","356000\n","357000\n","358000\n","359000\n","360000\n","361000\n","362000\n","363000\n","364000\n","365000\n","366000\n","367000\n","368000\n","369000\n","370000\n","371000\n","372000\n","373000\n","374000\n","375000\n","376000\n","377000\n","378000\n","379000\n","380000\n","381000\n","382000\n","383000\n","384000\n","385000\n","386000\n","387000\n","388000\n","389000\n","390000\n","391000\n","392000\n","done on training data  384 392702\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<__main__.SentenceDatasetRoberta at 0x7ff6f4788a58>"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"bCzKVvO4oa9R","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","\n","class LinearLayer(nn.Module):\n","\tdef __init__(self, batch_size=512, embedding_length=768, num_labels=3):\n","\t\tsuper(LinearLayer, self).__init__()\n","\n","\t\tself.fc = nn.Linear(embedding_length*2, 3)\n","\n","\tdef forward(self, premise, hypothesis):\n","\t\trep = torch.cat([premise, hypothesis], 1)\n","\t\treturn self.fc(rep)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VKQindscohaP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":75},"outputId":"a1221345-1a6e-4674-ea51-0dcf50fa7156","executionInfo":{"status":"ok","timestamp":1583561996459,"user_tz":-330,"elapsed":7657,"user":{"displayName":"Himadyuti Bhanja","photoUrl":"","userId":"06992247206842326199"}}},"source":["import os\n","import gc\n","import torch\n","import argparse\n","import numpy as np\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","import torch.optim as optim\n","import torch.backends.cudnn as cudnn\n","from torch.utils.data import DataLoader\n","import IPython\n","\n","parser = argparse.ArgumentParser(description='Experiment RobertaSTS Embeddings')\n","parser.add_argument('--lr', default=1e-4, type=float, help='learning rate') \n","parser.add_argument('--batch_size', default=1024, type=int) \n","parser.add_argument('--epochs', '-e', type=int, default=200, help='Number of epochs to train.')\n","parser.add_argument('--preparedata', type=int, default=1)\n","\n","args = parser.parse_args(args=[])\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","\n","print('==> Preparing data..')\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","print('==> Creating networks..')\n","llmodel = LinearLayer(batch_size = args.batch_size).to(device)\n","#llmodel.load_state_dict(torch.load(\"/content/drive/My Drive/Colab Notebooks/weights/networkrobertasts_train.ckpt\"))\n","params = llmodel.parameters()\n","optimizer = optim.Adam(params, lr=args.lr, weight_decay=1e-5)\n","\n","print('==> Loading data..')\n","trainset = SentenceDatasetRoberta()\n","testset = SentenceDatasetRoberta(test = True)\n","\n","def train_roberta(currepoch, epoch):\n","    dataloader = DataLoader(trainset, batch_size=args.batch_size, shuffle=True)\n","    dataloader = iter(dataloader)\n","    print('\\n=> Epoch: %d' % currepoch)\n","    \n","    train_loss, correct, total = 0, 0, 0\n","\n","    for batch_idx in range(len(dataloader)):\n","        premise, hypothesis, label = next(dataloader)\n","        if(batch_idx==len(dataloader)-1):\n","          continue\n","        premise, hypothesis, label = premise.to(device), hypothesis.to(device), label.to(device)\n","        optimizer.zero_grad()\n","        y_pred = llmodel(premise, hypothesis)\n","\n","        loss = criterion(y_pred, label)\n","        #print(y_pred, targets)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        _, predicted = y_pred.max(1)\n","        total += label.size(0)\n","        correct += predicted.eq(label).sum().item()\n","\n","        with open(\"/content/drive/My Drive/Colab Notebooks/logs/robertasts_train_loss.log\", \"a+\") as lfile:\n","            lfile.write(\"{}\\n\".format(train_loss / total))\n","\n","        with open(\"/content/drive/My Drive/Colab Notebooks/logs/robertasts_train_acc.log\", \"a+\") as afile:\n","            afile.write(\"{}\\n\".format(correct / total))\n","\n","        del premise\n","        del hypothesis\n","        del label\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","        torch.save(llmodel.state_dict(), '/content/drive/My Drive/Colab Notebooks/weights/networkrobertasts_train.ckpt')\n","        with open(\"/content/drive/My Drive/Colab Notebooks/information/robertasts_info.txt\", \"w+\") as f:\n","            f.write(\"{} {}\".format(currepoch, batch_idx))\n","        out.update(IPython.display.Pretty('Batch: [%d/%d], Loss: %.3f, Train Loss: %.3f , Acc: %.3f%% (%d/%d)' % (batch_idx, len(dataloader), loss.item(), train_loss/(batch_idx+1), 100.0*correct/total, correct, total)))\n","    torch.save(llmodel.state_dict(), '/content/drive/My Drive/Colab Notebooks/checkpoints/networkrobertasts_train_epoch_{}.ckpt'.format(currepoch + 1))\n","    print('\\n=> Classifier Network : Epoch [{}/{}], Loss:{:.4f}'.format(currepoch, epoch, train_loss / len(dataloader)))\n","\n","def test_roberta(currepoch, epoch):\n","    dataloader = DataLoader(testset, batch_size=args.batch_size, shuffle=True)\n","    dataloader = iter(dataloader)\n","    print('\\n=> Testing Epoch: %d' % currepoch)\n","    \n","    test_loss, correct, total = 0, 0, 0\n","\n","    for batch_idx in range(len(dataloader)):\n","        premise, hypothesis, label = next(dataloader)\n","        if(batch_idx==len(dataloader)-1):\n","          continue\n","          \n","        premise, hypothesis, label = premise.to(device), hypothesis.to(device), label.to(device)\n","        \n","        y_pred = llmodel(premise, hypothesis)\n","\n","        loss = criterion(y_pred, label)\n","\n","        test_loss += loss.item()\n","        _, predicted = y_pred.max(1)\n","        total += label.size(0)\n","        correct += predicted.eq(label).sum().item()\n","\n","        with open(\"/content/drive/My Drive/Colab Notebooks/logs/robertasts_test_loss.log\", \"a+\") as lfile:\n","            lfile.write(\"{}\\n\".format(test_loss / total))\n","\n","        with open(\"/content/drive/My Drive/Colab Notebooks/logs/robertasts_test_acc.log\", \"a+\") as afile:\n","            afile.write(\"{}\\n\".format(correct / total))\n","\n","        del premise\n","        del hypothesis\n","        del label\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","        out.update(IPython.display.Pretty('Batch: [%d/%d], Loss: %.3f, Test Loss: %.3f , Acc: %.3f%% (%d/%d)' % (batch_idx, len(dataloader), loss.item(), test_loss/(batch_idx+1), 100.0*correct/total, correct, total)))\n","\n","    print('\\n=> Classifier Network Test: Epoch [{}/{}], Loss:{:.4f}'.format(currepoch, epoch, test_loss / len(dataloader)))\n"],"execution_count":11,"outputs":[{"output_type":"stream","text":["==> Preparing data..\n","==> Creating networks..\n","==> Loading data..\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OieRWk0Cpcqf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"6da0e329-8ec6-43bc-8ced-5423ce0cf220"},"source":["out = display(IPython.display.Pretty('Starting'), display_id=True)\n","#test_roberta(0, args.epochs)\n","print('==> Training starts..')\n","for epoch in range(args.epochs):\n","    train_roberta(epoch, args.epochs)\n","    test_roberta(epoch, args.epochs)"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/plain":["Batch: [10/384], Loss: 0.817, Train Loss: 0.840 , Acc: 61.923% (6975/11264)"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["==> Training starts..\n","\n","=> Epoch: 0\n","\n","=> Classifier Network : Epoch [0/200], Loss:0.9159\n","\n","=> Testing Epoch: 0\n","\n","=> Classifier Network Test: Epoch [0/200], Loss:0.8114\n","\n","=> Epoch: 1\n","\n","=> Classifier Network : Epoch [1/200], Loss:0.8640\n","\n","=> Testing Epoch: 1\n","\n","=> Classifier Network Test: Epoch [1/200], Loss:0.8066\n","\n","=> Epoch: 2\n","\n","=> Classifier Network : Epoch [2/200], Loss:0.8569\n","\n","=> Testing Epoch: 2\n","\n","=> Classifier Network Test: Epoch [2/200], Loss:0.8028\n","\n","=> Epoch: 3\n","\n","=> Classifier Network : Epoch [3/200], Loss:0.8531\n","\n","=> Testing Epoch: 3\n","\n","=> Classifier Network Test: Epoch [3/200], Loss:0.7986\n","\n","=> Epoch: 4\n","\n","=> Classifier Network : Epoch [4/200], Loss:0.8505\n","\n","=> Testing Epoch: 4\n","\n","=> Classifier Network Test: Epoch [4/200], Loss:0.7983\n","\n","=> Epoch: 5\n","\n","=> Classifier Network : Epoch [5/200], Loss:0.8487\n","\n","=> Testing Epoch: 5\n","\n","=> Classifier Network Test: Epoch [5/200], Loss:0.7974\n","\n","=> Epoch: 6\n","\n","=> Classifier Network : Epoch [6/200], Loss:0.8472\n","\n","=> Testing Epoch: 6\n","\n","=> Classifier Network Test: Epoch [6/200], Loss:0.7948\n","\n","=> Epoch: 7\n","\n","=> Classifier Network : Epoch [7/200], Loss:0.8461\n","\n","=> Testing Epoch: 7\n","\n","=> Classifier Network Test: Epoch [7/200], Loss:0.7911\n","\n","=> Epoch: 8\n","\n","=> Classifier Network : Epoch [8/200], Loss:0.8451\n","\n","=> Testing Epoch: 8\n","\n","=> Classifier Network Test: Epoch [8/200], Loss:0.7939\n","\n","=> Epoch: 9\n","\n","=> Classifier Network : Epoch [9/200], Loss:0.8444\n","\n","=> Testing Epoch: 9\n","\n","=> Classifier Network Test: Epoch [9/200], Loss:0.7909\n","\n","=> Epoch: 10\n","\n","=> Classifier Network : Epoch [10/200], Loss:0.8435\n","\n","=> Testing Epoch: 10\n","\n","=> Classifier Network Test: Epoch [10/200], Loss:0.7923\n","\n","=> Epoch: 11\n","\n","=> Classifier Network : Epoch [11/200], Loss:0.8431\n","\n","=> Testing Epoch: 11\n","\n","=> Classifier Network Test: Epoch [11/200], Loss:0.7929\n","\n","=> Epoch: 12\n","\n","=> Classifier Network : Epoch [12/200], Loss:0.8426\n","\n","=> Testing Epoch: 12\n","\n","=> Classifier Network Test: Epoch [12/200], Loss:0.7903\n","\n","=> Epoch: 13\n","\n","=> Classifier Network : Epoch [13/200], Loss:0.8422\n","\n","=> Testing Epoch: 13\n","\n","=> Classifier Network Test: Epoch [13/200], Loss:0.7893\n","\n","=> Epoch: 14\n","\n","=> Classifier Network : Epoch [14/200], Loss:0.8418\n","\n","=> Testing Epoch: 14\n","\n","=> Classifier Network Test: Epoch [14/200], Loss:0.7900\n","\n","=> Epoch: 15\n","\n","=> Classifier Network : Epoch [15/200], Loss:0.8414\n","\n","=> Testing Epoch: 15\n","\n","=> Classifier Network Test: Epoch [15/200], Loss:0.7906\n","\n","=> Epoch: 16\n","\n","=> Classifier Network : Epoch [16/200], Loss:0.8412\n","\n","=> Testing Epoch: 16\n","\n","=> Classifier Network Test: Epoch [16/200], Loss:0.7880\n","\n","=> Epoch: 17\n","\n","=> Classifier Network : Epoch [17/200], Loss:0.8409\n","\n","=> Testing Epoch: 17\n","\n","=> Classifier Network Test: Epoch [17/200], Loss:0.7898\n","\n","=> Epoch: 18\n","\n","=> Classifier Network : Epoch [18/200], Loss:0.8406\n","\n","=> Testing Epoch: 18\n","\n","=> Classifier Network Test: Epoch [18/200], Loss:0.7888\n","\n","=> Epoch: 19\n","\n","=> Classifier Network : Epoch [19/200], Loss:0.8404\n","\n","=> Testing Epoch: 19\n","\n","=> Classifier Network Test: Epoch [19/200], Loss:0.7883\n","\n","=> Epoch: 20\n","\n","=> Classifier Network : Epoch [20/200], Loss:0.8402\n","\n","=> Testing Epoch: 20\n","\n","=> Classifier Network Test: Epoch [20/200], Loss:0.7907\n","\n","=> Epoch: 21\n","\n","=> Classifier Network : Epoch [21/200], Loss:0.8399\n","\n","=> Testing Epoch: 21\n","\n","=> Classifier Network Test: Epoch [21/200], Loss:0.7919\n","\n","=> Epoch: 22\n","\n","=> Classifier Network : Epoch [22/200], Loss:0.8398\n","\n","=> Testing Epoch: 22\n","\n","=> Classifier Network Test: Epoch [22/200], Loss:0.7889\n","\n","=> Epoch: 23\n","\n","=> Classifier Network : Epoch [23/200], Loss:0.8397\n","\n","=> Testing Epoch: 23\n","\n","=> Classifier Network Test: Epoch [23/200], Loss:0.7880\n","\n","=> Epoch: 24\n","\n","=> Classifier Network : Epoch [24/200], Loss:0.8394\n","\n","=> Testing Epoch: 24\n","\n","=> Classifier Network Test: Epoch [24/200], Loss:0.7868\n","\n","=> Epoch: 25\n","\n","=> Classifier Network : Epoch [25/200], Loss:0.8394\n","\n","=> Testing Epoch: 25\n","\n","=> Classifier Network Test: Epoch [25/200], Loss:0.7912\n","\n","=> Epoch: 26\n","\n","=> Classifier Network : Epoch [26/200], Loss:0.8392\n","\n","=> Testing Epoch: 26\n","\n","=> Classifier Network Test: Epoch [26/200], Loss:0.7881\n","\n","=> Epoch: 27\n","\n","=> Classifier Network : Epoch [27/200], Loss:0.8392\n","\n","=> Testing Epoch: 27\n","\n","=> Classifier Network Test: Epoch [27/200], Loss:0.7887\n","\n","=> Epoch: 28\n","\n","=> Classifier Network : Epoch [28/200], Loss:0.8390\n","\n","=> Testing Epoch: 28\n","\n","=> Classifier Network Test: Epoch [28/200], Loss:0.7890\n","\n","=> Epoch: 29\n","\n","=> Classifier Network : Epoch [29/200], Loss:0.8389\n","\n","=> Testing Epoch: 29\n","\n","=> Classifier Network Test: Epoch [29/200], Loss:0.7887\n","\n","=> Epoch: 30\n","\n","=> Classifier Network : Epoch [30/200], Loss:0.8389\n","\n","=> Testing Epoch: 30\n","\n","=> Classifier Network Test: Epoch [30/200], Loss:0.7893\n","\n","=> Epoch: 31\n","\n","=> Classifier Network : Epoch [31/200], Loss:0.8387\n","\n","=> Testing Epoch: 31\n","\n","=> Classifier Network Test: Epoch [31/200], Loss:0.7894\n","\n","=> Epoch: 32\n","\n","=> Classifier Network : Epoch [32/200], Loss:0.8387\n","\n","=> Testing Epoch: 32\n","\n","=> Classifier Network Test: Epoch [32/200], Loss:0.7884\n","\n","=> Epoch: 33\n","\n","=> Classifier Network : Epoch [33/200], Loss:0.8386\n","\n","=> Testing Epoch: 33\n","\n","=> Classifier Network Test: Epoch [33/200], Loss:0.7865\n","\n","=> Epoch: 34\n","\n","=> Classifier Network : Epoch [34/200], Loss:0.8385\n","\n","=> Testing Epoch: 34\n","\n","=> Classifier Network Test: Epoch [34/200], Loss:0.7914\n","\n","=> Epoch: 35\n","\n","=> Classifier Network : Epoch [35/200], Loss:0.8384\n","\n","=> Testing Epoch: 35\n","\n","=> Classifier Network Test: Epoch [35/200], Loss:0.7864\n","\n","=> Epoch: 36\n","\n","=> Classifier Network : Epoch [36/200], Loss:0.8384\n","\n","=> Testing Epoch: 36\n","\n","=> Classifier Network Test: Epoch [36/200], Loss:0.7874\n","\n","=> Epoch: 37\n","\n","=> Classifier Network : Epoch [37/200], Loss:0.8384\n","\n","=> Testing Epoch: 37\n","\n","=> Classifier Network Test: Epoch [37/200], Loss:0.7875\n","\n","=> Epoch: 38\n"],"name":"stdout"}]}]}